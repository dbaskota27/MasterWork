{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844c7669",
   "metadata": {},
   "source": [
    "# FTE requirements for NEXT 6 to 9 Months Forecasted\n",
    "# Only creates FTE Projections for the days and Time provided in HOOPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203822ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Key Features\n",
    "# üìû Call Volume and AHT Forecasting:\n",
    "\n",
    "# Predicts both call volumes and AHT using historical trends, time-of-day effects, weekly cycles, and seasonality.\n",
    "\n",
    "# Handles missing or incomplete data by referencing previous week‚Äôs data for the same time intervals.\n",
    "\n",
    "# üìÖ Time Series Feature Engineering:\n",
    "\n",
    "# Extracts features like day-of-week, month, year, and holidays.\n",
    "\n",
    "# Includes lag features: values from prior weeks and the same time last year to model seasonal patterns.\n",
    "\n",
    "# üóìÔ∏è Holiday/Hoops Awareness:\n",
    "\n",
    "# Detects U.S. Federal Holidays and Easter to adjust forecasts for special dates.\n",
    "# Only creates FTE Projections for the days and Time provided in HOOPS\n",
    "\n",
    "# üß† ML-Based Forecasting:\n",
    "\n",
    "# Trains regression models on historical features to predict future AHT and call volumes.\n",
    "\n",
    "# Flexible design allows for incorporating any sklearn-compatible ML model.\n",
    "\n",
    "# üìê FTE Estimation with Erlang-C:\n",
    "\n",
    "# Calculates the number of required agent positions using Erlang-C queuing theory.\n",
    "\n",
    "# Considers SLA goals, shrinkage, occupancy caps, and ASA thresholds per client.\n",
    "\n",
    "# üìà Call Pattern Reconstruction:\n",
    "\n",
    "# Dynamically builds future timeframes (test sets) based on client hoop schedules.\n",
    "\n",
    "# Applies statistical smoothing (rolling average) to stabilize noisy staffing values.\n",
    "\n",
    "# üóÑÔ∏è SQL Database Integration:\n",
    "\n",
    "# Pulls historical and client configuration data from SQL.\n",
    "\n",
    "# Inserts/upserts predictions back into the reporting DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Contact Center Forecasting Tool\n",
    "\n",
    "# This Jupyter Notebook is a powerful workforce management tool for **predicts call volumes, average handle times (AHT), call arrival patterns (CAP), and staffing requirements (FTE)** for contact centers. Using **machine learning (RandomForestRegressor)**, **Erlang-C queuing theory**, and **SQL database integration**, it provides **9-month forecasts** for staffing, shift planning, and performance analysis. The code features:\n",
    "\n",
    "# - **Call Arrival Pattern (CAP)**: Predicts call distribution across 30-minute intervals, handling holidays and missing data.\n",
    "# - **Call Volume Forecasting**: Projects daily call volumes with cyclical features and outlier handling.\n",
    "# - **AHT Forecasting**: Estimates handle times in 2-hour intervals.\n",
    "# - **FTE Estimation**: Calculates required agents using Erlang-C, considering SLAs, shrinkage, and occupancy.\n",
    "# - **Visualization**: Plots year-over-year call volume trends.\n",
    "# - **Database Integration**: Pulls/stores data via SQL for scalability.\n",
    "\n",
    "# Built with **pandas**, **sklearn**, **matplotlib**, and **pyodbc**, this tool supports **workforce management**, **analysts**, and **operations leadership** in optimizing contact center operations. Requires Avaya and reporting databases.\n",
    "\n",
    "# **Note**: Some SQL queries need customization, and projections are best for up to 6 months.\n",
    "\n",
    "# ## Setup\n",
    "# 1. Install dependencies: `pip install pandas numpy sklearn matplotlib pyodbc holidays python-dateutil boto3 sendgrid jinja2`.\n",
    "# 2. Configure database connections (`get_db_avaya_stats`, `get_db_dictionary`, `get_db_Client_data_import`).\n",
    "# 3. Run the notebook with appropriate client codes, team names, and date ranges.\n",
    "\n",
    "# ## Usage\n",
    "# Modify `ds`, `de`, `Clientcode`, and `teams` to run forecasts. Use `plot_yoy_call_volume_with_2025` for visualizations. Outputs are stored in the database for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e93cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose\n",
    "# The primary purpose of this notebook is to provide contact center analysts, workforce management teams, and operations leadership with data-driven insights for:\n",
    "\n",
    "# Staffing Projections: Forecast required agent positions based on predicted call volumes, AHT, and service level agreements (SLAs).\n",
    "# Shift Planning: Optimize schedules using call arrival patterns and hours of operation (HoOPS).\n",
    "# Performance Analysis: Evaluate historical call data and simulate SLA scenarios.\n",
    "# Strategic Decisions: Support hiring, outsourcing, and resource allocation decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc366ffb",
   "metadata": {},
   "source": [
    "# This block of code Below gets the Call Arrival Pattern for future projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ddb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import xlsxwriter\n",
    "import os\n",
    "import pyodbc \n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import holidays\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "import jinja2\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import urllib.parse\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import sendgrid\n",
    "from sendgrid.helpers.mail import *\n",
    "import json\n",
    "import urllib\n",
    "import boto3\n",
    "from boto.s3.key import Key\n",
    "import base64\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_data(client, team=None):\n",
    "    # Connect to the Avaya statistics database\n",
    "    avaya_db = get_db_avaya_stats()\n",
    "    \n",
    "    # If a team is provided, fetch call data for the specific client and team\n",
    "    if team:\n",
    "        sql = \"\"\"Query to pull data\"\"\"\n",
    "        sql = sql % (client, team)\n",
    "    # If no team is provided, fetch call data for the entire client\n",
    "    else:\n",
    "        sql = \"\"\"Query to pull data\"\"\"\n",
    "        sql = sql % (client)\n",
    "    \n",
    "    # Execute the SQL query and load the result into a pandas DataFrame\n",
    "    data = pd.read_sql(sql, con=avaya_db)\n",
    "    \n",
    "    # Sort the data chronologically by the 'Date' column\n",
    "    data = data.sort_values(by='Date', ascending=True)\n",
    "    \n",
    "    # Exclude data for today's date\n",
    "    today_date = datetime.today().date()\n",
    "    data = data[data['Date'] != today_date]\n",
    "\n",
    "    # Return an empty list if no data was retrieved\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    # Convert the 'Time' column from timedelta to an integer format\n",
    "    data = time_to_int(data)\n",
    "    \n",
    "    # Generate a complete time series from the first to last date,\n",
    "    # covering every 30-minute interval in a day\n",
    "    rng = get_date_range(data.Date.iloc[0], data.Date.iloc[-1])\n",
    "\n",
    "    # Print data types for debugging purposes\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    # Merge the actual data with the complete date-time range to fill in missing time intervals\n",
    "    df = pd.merge(rng, data, how=\"outer\", on=[\"Date\", \"Time\"])\n",
    "    \n",
    "    # Fill any missing values (NaNs) resulting from the merge with 0s\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Ensure the 'Calls' column is of integer type\n",
    "    df['Calls'] = df['Calls'].astype(int)\n",
    "    \n",
    "    # Perform feature engineering on the dataset to add model-specific features\n",
    "    df = feature_engineering(df, team)\n",
    "    \n",
    "    # Return the final dataframe ready for use in a machine learning model\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6116acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_calls(df, team):\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Sort the DataFrame by 'Date' and reset index\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Special handling for the \"Technical Support\" team\n",
    "    if team == \"Technical Support\":\n",
    "        # Consider only weekdays (Monday to Friday)\n",
    "        weekday_calls = (df['day_of_week'] < 5)\n",
    "        \n",
    "        # Ensure 'Time' column is in integer format\n",
    "        df['Time'] = df['Time'].astype(int)\n",
    "\n",
    "        # Identify 30-min intervals between 6:00 PM and 8:00 PM with 0 calls\n",
    "        evening_calls_zero = (df['Time'] >= 1800) & (df['Time'] <= 2000) & (df['Calls'] == 0)\n",
    "\n",
    "        # Set 'Calls' to 1 for identified time slots during weekdays\n",
    "        df.loc[weekday_calls & evening_calls_zero, 'Calls'] = 1\n",
    "\n",
    "    # Loop through each row to fill 0-call slots based on same time interval from the previous week\n",
    "    for index, row in df.iterrows():\n",
    "        # Focus only on weekdays and where Calls == 0\n",
    "        if row['day_of_week'] < 5 and row['Calls'] == 0:\n",
    "            current_date = row['Date']\n",
    "            current_time = row['Time']\n",
    "\n",
    "            # Calculate the same day one week before\n",
    "            prev_week_same_day = current_date - pd.DateOffset(weeks=1)\n",
    "\n",
    "            # Fetch the corresponding row from the previous week for the same time\n",
    "            prev_week_data = df[\n",
    "                (df['Date'] == prev_week_same_day) &\n",
    "                (df['Time'] == current_time)\n",
    "            ]\n",
    "\n",
    "            # If such a row exists, replace the current 0-call value with the previous week's call value\n",
    "            if not prev_week_data.empty:\n",
    "                df.at[index, 'Calls'] = prev_week_data.iloc[0]['Calls']\n",
    "\n",
    "    # Convert 'Date' column back to just date format (dropping time component)\n",
    "    df['Date'] = df['Date'].dt.date\n",
    "\n",
    "    # Return the cleaned and updated DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe containing every time interval for everyday in the date range first to last.\n",
    "# What is returned is a complete time series containing zero gaps.\n",
    "# Params 'first' and 'last' are both datetime.date objects\n",
    "\n",
    "def get_date_range(first, last):\n",
    "    # Initialize an empty DataFrame with the expected columns\n",
    "    df = pd.DataFrame(columns=[\"Date\", \"Time\"])\n",
    "    \n",
    "    # Generate a list of dates between 'first' and 'last' (inclusive)\n",
    "    rng = pd.date_range(start=first, end=last).date\n",
    "    rng = pd.DataFrame(rng, columns=['Date'])\n",
    "\n",
    "    # Iterate over each date in the range\n",
    "    for i in range(len(rng)):\n",
    "        d = rng.Date[i]\n",
    "        arr = []\n",
    "\n",
    "        # Generate 30-minute intervals: 00:00, 00:30, ..., 23:30 represented as integers (e.g., 0, 30, ..., 2330)\n",
    "        for i in range(24):\n",
    "            arr.append(i*100)       # e.g., 0000, 0100, ..., 2300\n",
    "            arr.append(i*100 + 30)  # e.g., 0030, 0130, ..., 2330\n",
    "\n",
    "        # Create a DataFrame with current date and its corresponding time intervals\n",
    "        temp = pd.DataFrame(arr, columns=[\"Time\"])\n",
    "        temp[\"Date\"] = d\n",
    "\n",
    "        # Append the new rows to the main DataFrame\n",
    "        df = pd.concat([df, temp])\n",
    "    \n",
    "    # Return the complete date-time range DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the Time column to an integer. \n",
    "def time_to_int(df):\n",
    "    df[\"Time\"] = df[\"Time\"].astype(str)\n",
    "    df[\"Time\"] = df.apply(lambda row : row[\"Time\"][-8:-3], axis=1)\n",
    "    df[\"Time\"] = df.apply(lambda row : row[\"Time\"].replace(\":\", \"\"), axis=1)\n",
    "    df[\"Time\"] = df[\"Time\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for get_call_data(). Sets the holiday flag to true for all Easter Sundays and Good Fridays in df.\n",
    "def set_easter_holiday(df):\n",
    "    easters = []\n",
    "    year = df[\"Date\"].iloc[0].year\n",
    "    while year <= datetime.now().year:\n",
    "        d = date(year,1,1)\n",
    "        e = d + pd.offsets.Easter()\n",
    "        e = e.date()\n",
    "        gf = e - timedelta(2)\n",
    "        df.loc[df[\"Date\"] == e, \"is_holiday\"] = 1\n",
    "        df.loc[df[\"Date\"] == gf, \"is_holiday\"] = 1\n",
    "        year = year + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe39b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the column \"Percent\" to the dataframe, where in each row, \"Percent\" is the percentage of total call volume \n",
    "# for that date. \n",
    "def addPercent(df):\n",
    "    sums = df.groupby([\"Date\"])[[\"Calls\"]].sum()\n",
    "    def addPercentHelper(row, df):\n",
    "        p = row.Calls\n",
    "        q = sums[sums.index == row.Date].Calls.item()\n",
    "        if q == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return p/q\n",
    "    \n",
    "    df[\"Percent\"] = df.apply(lambda row : addPercentHelper(row, sums), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16279756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    sums = df.groupby([\"Date\"])[[\"Predictions\"]].sum()\n",
    "    def normalizeHelper(row, df):\n",
    "        p = row.Predictions\n",
    "        q = sums[sums.index == row.Date].Predictions.item()\n",
    "        if q == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return p/q\n",
    "    \n",
    "    df[\"CAP\"] = df.apply(lambda row : normalizeHelper(row, sums), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c526403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the number of calls from the (depth) most recent week.\n",
    "# For future dates, which haven't yet occurred, this function grabs call data from the most current \n",
    "# weeks of data.\n",
    "def get_last_week(depth, day, time, old_data):\n",
    "#     print (day,\"Day\")\n",
    "    old_data['Date'] = pd.to_datetime(old_data['Date'])\n",
    "\n",
    "    old_data['Date'] = old_data['Date'].dt.date\n",
    "\n",
    "#     print(old_data.Date.iloc[-1],\"last date\")\n",
    "#     print(time)\n",
    "    while day > old_data.Date.iloc[-1]:\n",
    "        day = day - timedelta(7)\n",
    "    \n",
    "    d = day-timedelta( 7 * (depth-1) )\n",
    "    return old_data[(old_data.Date == d) & (old_data.Time == time)].Percent.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hoops_for_test_frame(team,start_date, end_date):\n",
    "    from datetime import date, timedelta, datetime\n",
    "    import datetime as dt\n",
    "\n",
    "    todays_date = datetime.today().date()\n",
    "    sql = \"\"\"My query to get Hours of Operations from the database Table\"\"\"\n",
    "    sql = sql % (team,start_date,end_date,team,start_date,end_date)\n",
    "\n",
    "    Hoops = pd.read_sql(sql, con=reporting_db)\n",
    "    Hoops['Hoop_Start']= pd.to_datetime(Hoops['Hoop_Start'])\n",
    "    Hoops['Hoop_Start'] = pd.to_datetime(Hoops['Hoop_Start']).dt.time\n",
    "\n",
    "    Hoops['Hoop_End']= pd.to_datetime(Hoops['Hoop_End'])\n",
    "    Hoops['Hoop_End'] = pd.to_datetime(Hoops['Hoop_End']).dt.time\n",
    "\n",
    "    total_rows = Hoops.shape[0]\n",
    "\n",
    "    # total_rows = 15\n",
    "\n",
    "# Initialize a list to accumulate the rows\n",
    "    rows = []\n",
    "\n",
    "    for x in range(total_rows):\n",
    "        PIT_Date = Hoops['PIT_Date'][x]\n",
    "        Hoop_Start = str(Hoops['Hoop_Start'][x])\n",
    "        Hoop_End = str(Hoops['Hoop_End'][x])\n",
    "\n",
    "        # Convert Hoop_Start and Hoop_End from strings to datetime objects\n",
    "        start = dt.datetime.strptime(Hoop_Start, '%H:%M:%S')\n",
    "        end = dt.datetime.strptime(Hoop_End, '%H:%M:%S')\n",
    "        delta = dt.timedelta(minutes=30)\n",
    "\n",
    "        # Loop through the time range\n",
    "        t = start\n",
    "        while t < end:\n",
    "            Daterec = t.strftime('%H:%M:%S')\n",
    "            t += delta\n",
    "            # Append each row to the list as a dictionary\n",
    "            rows.append({'Date_PIT': PIT_Date, 'Timestamp': Daterec})\n",
    "\n",
    "    # Create the Final DataFrame from the list of dictionaries\n",
    "    Finaldataframe = pd.DataFrame(rows)\n",
    "    \n",
    "    df1 = Finaldataframe\n",
    "    df1[\"Timestamp\"] = df1[\"Timestamp\"].astype(str)\n",
    "    df1[\"Timestamp\"] = df1.apply(lambda row : row[\"Timestamp\"][:-3], axis=1)\n",
    "    df1[\"Timestamp\"] = df1.apply(lambda row : row[\"Timestamp\"].replace(\":\", \"\"), axis=1)\n",
    "    df1[\"Timestamp\"] = df1[\"Timestamp\"].astype(int)\n",
    "    df1 = df1.rename(columns={'Timestamp': 'Time'})\n",
    "    df1 = df1.rename(columns={'Date_PIT': 'Date'})\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe for the future projection, containing all feature engineering columns needed \n",
    "# for the machine learning model.\n",
    "def create_test_frame(team,start_date, end_date, old_data):\n",
    "    print(\"\\tCreating the test frame...\")\n",
    "    # Start out with a dataframe containing all days and time intervals from start_date to end_date\n",
    "    rng = get_hoops_for_test_frame(team,start_date, end_date)\n",
    "\n",
    "    \n",
    "    # The day of the week for each row is important information\n",
    "    rng[\"day_of_week\"] = rng.apply(lambda row : row[\"Date\"].weekday(), axis=1)\n",
    "    \n",
    "    # Extract information about the date, because a date can not be fed to machine learning model. Only numbers.\n",
    "    rng[\"month\"] = rng.apply(lambda row: row[\"Date\"].month, axis = 1)\n",
    "    rng[\"day_of_month\"] = rng.apply(lambda row: row[\"Date\"].day, axis = 1)\n",
    "    rng[\"year\"] = rng.apply(lambda row: row[\"Date\"].year, axis = 1)\n",
    "    \n",
    "    # Is date a US Holiday?\n",
    "    us_holidays = holidays.US()\n",
    "    rng[\"is_holiday\"] = rng.apply(lambda row : row[\"Date\"] in us_holidays, axis=1 )\n",
    "    rng[\"is_holiday\"] = rng[\"is_holiday\"].astype(int)\n",
    "    rng = set_easter_holiday(rng)\n",
    " \n",
    "    # Get the 3 most recent weeks of data that are not in the future\n",
    "    rng[\"last_week\"] = rng.apply(lambda row : get_last_week(1, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "    rng[\"last_week_2\"] = rng.apply(lambda row : get_last_week(2, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "    rng[\"last_week_3\"] = rng.apply(lambda row : get_last_week(3, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "    \n",
    "    # \"Train\" dataframes containing small amounts of historic data should not be given last_year columns, because \n",
    "    # the data doesn't extend that far into the past. Instead, they should be fitted to the ML Model without\n",
    "    # the last_year information columns, and only with previous week data, in order to maximize the amount of rows\n",
    "    # fed to the ML model. Train and Test frames must contain all the same columns, thus, we check if the train frame\n",
    "    # (old_data) has old_year columns. If it does not, then it is a \"short\" frame, containing only past weeks data.\n",
    "    if \"last_year\" in old_data.columns:\n",
    "        # Get the 4th-9th most recent weeks of data\n",
    "        rng[\"last_week_4\"] = rng.apply(lambda row : get_last_week(4, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_5\"] = rng.apply(lambda row : get_last_week(5, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_6\"] = rng.apply(lambda row : get_last_week(6, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_7\"] = rng.apply(lambda row : get_last_week(7, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_8\"] = rng.apply(lambda row : get_last_week(8, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_9\"] = rng.apply(lambda row : get_last_week(9, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        \n",
    "        # Get data from this day and time last year, plus and minus 1 week\n",
    "        rng[\"last_year\"] = rng.apply(lambda row : old_data[(old_data.Date == row.Date - timedelta(364)) & (old_data.Time == row.Time)].Percent.item(), axis=1)\n",
    "        rng[\"last_year-1w\"] = rng.apply(lambda row : old_data[(old_data.Date == row[\"Date\"] - timedelta(371)) & (old_data.Time == row.Time)].Percent.item(), axis=1)\n",
    "        rng[\"last_year+1w\"] = rng.apply(lambda row : old_data[(old_data.Date == row[\"Date\"] - timedelta(357)) & (old_data.Time == row.Time)].Percent.item(), axis=1)\n",
    "        \n",
    "        rng = rng[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \"last_week_2\", \"last_week_3\", \"last_week_4\", \"last_week_5\", \"last_week_6\", \"last_week_7\", \"last_week_8\", \"last_week_9\", \"last_year\", \"last_year-1w\", \"last_year+1w\"]]\n",
    "    # This is for \"short\" dataframes\n",
    "    else:\n",
    "        # The full test frame has the following columns:\n",
    "        rng = rng[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \"last_week_2\", \"last_week_3\"]]\n",
    "    return rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5040d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given start_date and end_date, generates a dataframe of predictions for each day\n",
    "# in the date range.\n",
    "# Limitation: Projections work best for up to 6 months in the future, and should only be used for future dates.\n",
    "def future_projection(start_date, end_date, client, team, model):\n",
    "    print (start_date)\n",
    "    \n",
    "    if isinstance(start_date, str):\n",
    "        # Convert string to datetime\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        \n",
    "    elif isinstance(start_date, date):\n",
    "        # No change needed\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"start_date must be a string or a date object\")\n",
    "\n",
    "\n",
    "    # Collect all historic data, with feature engineered columns\n",
    "    df = get_call_data(client, team)\n",
    "\n",
    "    if len(df) == 0: \n",
    "        return []\n",
    "    if df.iloc[-1].Date < date.today() - timedelta(21):\n",
    "        return []\n",
    "    print(\"\\tFitting model with historic data...\")\n",
    "    \n",
    "    # Check if the historic data uses last_year metrics as a predictor. This means there was a sufficient\n",
    "    # amount of historic data to use the prior year's data as a predictor for the the future. Otherwise it \n",
    "    # is a \"short\" dataframe and will only use prior weeks of data as a predictor.\n",
    "    if \"last_year\" in df.columns and df.iloc[0].Date <= start_date - timedelta(371):\n",
    "        # Fit the model to the training frame by selecting all the columns except for \"Percent\", which is the \n",
    "        # metric the model is attempting to predict\n",
    "        model.fit(df[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \n",
    "                         \"last_week_2\", \"last_week_3\", \"last_week_4\", \"last_week_5\", \"last_week_6\", \n",
    "                         \"last_week_7\", \"last_week_8\", \"last_week_9\", \"last_year\", \"last_year-1w\", \n",
    "                         \"last_year+1w\"]], df[\"Percent\"])\n",
    "\n",
    "    # Enter the else block if df is a \"short\" frame which does not use prior year's data\n",
    "    else:\n",
    "        # Fit the model to the training frame by taking all columns except the one we are trying to predict. \n",
    "        # Notice the \"short\" training frame does not have last_year metrics\n",
    "        model.fit(df[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \n",
    "                         \"last_week_2\", \"last_week_3\"]], df[\"Percent\"])\n",
    "   \n",
    "    # Create the test frame, including all feature engineered columns used to fit the model\n",
    "    test = create_test_frame(team,start_date, end_date, df)\n",
    "    test.fillna(0, inplace=True)\n",
    "#     print(test)\n",
    "    # Create the prediction series\n",
    "    print(\"\\tGenerating predictions...\")\n",
    "    pred = model.predict(test)\n",
    "    pred = pd.DataFrame(pred, columns=[\"Predictions\"])\n",
    "    \n",
    "    # We want to return a dataframe with the date, time, and the predicted call volume for that date, \n",
    "    # but right now we only have a series of numbers and no dates.\n",
    "    # Create a dataframe containing all dates and time intervals from start_date to end_date\n",
    "    rng = get_hoops_for_test_frame(team,start_date, end_date)\n",
    "    rng[\"day_of_week\"] = rng.apply(lambda row : row.Date.weekday(), axis=1)\n",
    "    # Then concatenate the predictions with the date range dataframe, and voila, this is what we return\n",
    "    pred = pd.concat([pred.reset_index(drop=True), rng.reset_index(drop=True)], axis=1)\n",
    "    # Ensure that weekends are filled with 0 percents, since there is no call volume on weekends\n",
    "    # Secondly, normalize the CAPs so that the sum(Predictions) for each day is equal to 1.0\n",
    "    pred[\"Predictions\"].loc[pred[\"day_of_week\"] > 4] = 0\n",
    "    pred = normalize(pred)\n",
    "    # Return the predictions for the team for the given date range\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23759160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Update statement exists to set the Latest_Version to 0 for the team that is being predicted\n",
    "# The Insert statement inserts the latest new latest prediction for the given team \n",
    "insert_sql = \"\"\"insert statement \"\"\"\n",
    "D = \"ON DUPLICATE KEY UPDATE CAP_Percent1 = Values(CAP_Percent1),CAP_Percent2=Values(CAP_Percent2),Version_timestmp=Values(Version_timestmp)\\n\"\n",
    "\n",
    "# For each team in the teams array, get the prediction dataframe, and perform the update, and insert queries\n",
    "for team in teams:\n",
    "    print(\"Generating CAP Predictions for\",team, \"...\")\n",
    "    # Get the prediction dataframe for team, then execute update_sql for team\n",
    "    pred = future_projection(ds, de, client, team, model)\n",
    "    if len(pred) == 0:\n",
    "        print(\"\\tDue to lack of data, \" + team + \" is being skipped\")\n",
    "        continue\n",
    "#     sql = update_sql % (team)\n",
    "#     run_query(sql)\n",
    "    print(\"\\tInserting predictions into the database\")\n",
    "    \n",
    "    # Add rows of the prediction dataframe to the insert query. Every 500 rows, execute the query to prevent\n",
    "    # it from becoming too long.\n",
    "    sql = insert_sql\n",
    "    i = 0  # keeps track of multiples of 500\n",
    "    ct = str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    for index, row in pred.iterrows():\n",
    "        if pred.iloc[index, 0] == 0:\n",
    "            continue\n",
    "        pkey = str(client)+ \"-\" + str(team)+ \"-\" + str(pred.iloc[index, 1])+ \"-\" + str(pred.iloc[index, 2]*100) + \"-\" + str(VersionType)\n",
    "        s = \"('%s','%s', '%s', 'phone','%s','%s', '%s', %s, %s, 0, 0),\\n\"\n",
    "        s = s % (pkey,client, team,VersionType, ct, str(pred.iloc[index, 1]), pred.iloc[index, 2]*100, pred.iloc[index, 4])\n",
    "        sql += s\n",
    "        i += 1\n",
    "        # If we reach a multiple of 500, it is time to insert the 500 rows we have added to the query, then start anew\n",
    "        if i == 500:\n",
    "            sql = sql[:-2]\n",
    "            sql += \"\\n\" + D\n",
    "            run_query(sql)\n",
    "            i = 0\n",
    "            sql = insert_sql\n",
    "    \n",
    "    # Unless the length of the dataframe is a multiple of 500, then there will be rows of the dataframe that\n",
    "    # have not been inserted into the DB. Here we insert the remaining rows, if they exist.\n",
    "    if sql != insert_sql:\n",
    "        sql = sql[:-2]\n",
    "        sql += \"\\n\" + D\n",
    "        run_query(sql)\n",
    "    print(\"\\tFinished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa8b14",
   "metadata": {},
   "source": [
    "## This block of code below gets the Call volume future projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51565d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given client code and optional team name, returns a dataframe\n",
    "# containing call volume data per day, fully prepped for the ML model\n",
    "def get_call_data(clientCode, team):\n",
    "    avaya_db = get_db_avaya_stats()\n",
    "    # Query for getting a team's call data\n",
    "    if team:\n",
    "        sql = \"\"\"select DATE(row_date) as Date, \n",
    "        sum(i_arrived) as 'Calls'from avaya_stats.hsplit h\n",
    "        join reporting.avaya_skills_dict s on s.split = h.split\n",
    "        where client = '%s' and Forecasting_Skillname = \"%s\" and row_date >= \"2022-01-01\" \n",
    "        group by row_date\"\"\"\n",
    "        sql = sql % (clientCode, team)\n",
    "    # Query for getting a client's call data\n",
    "    else:\n",
    "        sql = \"\"\"Select DATE(row_date) as 'Date',\n",
    "        sum(A.i_arrived) as 'Calls' from avaya_stats.hvdn A\n",
    "        join reporting.avaya_vdn_desc D on D.vdn = A.vdn\n",
    "        where D.clientCode = '%s' and D.VDN_Type = 'Incoming ANI' and row_date >= \"2022-01-01\"\n",
    "        group by row_date\"\"\"\n",
    "        sql = sql % (clientCode)\n",
    "        \n",
    "    df = pd.read_sql(sql, con=avaya_db)\n",
    "    df = df.sort_values(by='Date', ascending=True)\n",
    "    today_date = datetime.today().date()\n",
    "    today_date = today_date-timedelta(1)\n",
    "    df = df[df['Date'] < today_date]\n",
    "    df = df[df[\"Calls\"]>10]    \n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Team and skill data needs to be changed to fit model\n",
    "    if team:\n",
    "        df = pad_call_data(df)\n",
    "    \n",
    "    #adds all feature engineering columns that are useful to the ML model\n",
    "    df = add_feature_columns(df)\n",
    "    \n",
    "    return df\n",
    "import pandas as pd\n",
    "# Importing the pandas library\n",
    "def fill_missing_calls(df):\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    # Sort the DataFrame by 'Date'\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['day_of_week'] < 5 and row['Calls'] == 0:\n",
    "            current_date = row['Date']\n",
    "            week_start = current_date - pd.DateOffset(days=current_date.weekday())\n",
    "\n",
    "            # weeks end on Friday (weekday 4)\n",
    "            week_end = week_start + pd.DateOffset(days=4)  \n",
    "            # Filter data for the current week and calculate average 'Calls'\n",
    "            current_week_data = df[(df['Date'] >= week_start) & (df['Date'] <= week_end)]['Calls']\n",
    "            recent_week_data = df[(df['Date'] >= week_start - pd.DateOffset(weeks=1)) & (df['Date'] <= week_end - pd.DateOffset(weeks=1))]['Calls']\n",
    "\n",
    "            # If data for the current week is available, calculate its average, else use the average of the most recent week\n",
    "            if not current_week_data.empty:\n",
    "                avg_calls = current_week_data[current_week_data != 0].mean()\n",
    "            else:\n",
    "                avg_calls = recent_week_data[recent_week_data != 0].mean()\n",
    "\n",
    "            # Fill missing 'Calls' value with the calculated average\n",
    "            df.at[index, 'Calls'] = int(avg_calls) if not pd.isnull(avg_calls) else 0\n",
    "\n",
    "    return df\n",
    "def add_feature_columns(df):\n",
    "    short = False\n",
    "    if df.iloc[0].Date > df.iloc[-1].Date - timedelta(371*2): short = True\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"day_of_week\"] = df[\"Date\"].dt.weekday\n",
    "    df.loc[df[\"day_of_week\"] > 4, \"Calls\"] = 0\n",
    "    df[\"month\"] = df[\"Date\"].dt.month\n",
    "    df[\"day_of_month\"] = df[\"Date\"].dt.day\n",
    "    df[\"year\"] = df[\"Date\"].dt.year\n",
    "    us_holidays = holidays.US()\n",
    "    df[\"is_holiday\"] = df[\"Date\"].apply(lambda x: int(x in us_holidays))\n",
    "    df[\"is_holiday\"] = df[\"is_holiday\"].astype(int)\n",
    "    df = set_easter_holiday(df)\n",
    "    df = fill_missing_calls(df)\n",
    "    \n",
    "    df['day_of_year'] = df['Date'].map(lambda x: x.timetuple().tm_yday)\n",
    "    df['year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df['year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df[\"last_week\"] = df[\"Calls\"].shift(7)\n",
    "    df[\"last_week_2\"] = df[\"Calls\"].shift(14)\n",
    "    df[\"last_week_3\"] = df[\"Calls\"].shift(21)\n",
    "    \n",
    "    df['rolling_avg_28d'] = df['Calls'].rolling(window=28, min_periods=1).mean().shift(1)\n",
    "    \n",
    "    if not short:\n",
    "        df[\"last_week_4\"] = df[\"Calls\"].shift(28)\n",
    "        df.loc[:, \"current_avg\"] = (df[\"last_week\"] + df[\"last_week_2\"] + df[\"last_week_3\"] + df[\"last_week_4\"])/4\n",
    "        df.loc[:, \"last_year\"] = df[\"Calls\"].shift(364)\n",
    "        df.loc[:, \"last_year-1w\"] = df[\"Calls\"].shift(371)\n",
    "        df.loc[:, \"last_year+1w\"] = df[\"Calls\"].shift(357)\n",
    "        df.loc[:, \"historic_avg\"] = (df[\"last_year\"] + df[\"last_year-1w\"] + df[\"last_year+1w\"])/3\n",
    "        df.loc[:, \"corr\"] = (df[\"current_avg\"] - df[\"historic_avg\"])/df[\"historic_avg\"]\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        df_new = df[371:].copy()\n",
    "        df_new = df_new.fillna(0)\n",
    "        df = df_new\n",
    "    else:\n",
    "        df_new = df.dropna().copy()\n",
    "        df = df_new\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Helper function for get_call_data(). Sets the holiday flag to true for all Easter Sundays and Good Fridays in df.\n",
    "def set_easter_holiday(df):\n",
    "    easters = []\n",
    "    year = df[\"Date\"].iloc[0].year\n",
    "    while year <= datetime.now().year:\n",
    "        d = date(year,1,1)\n",
    "        e = d + pd.offsets.Easter()\n",
    "        e = e.date()\n",
    "        gf = e - timedelta(2)\n",
    "        df.loc[df[\"Date\"] == e, \"is_holiday\"] = 1\n",
    "        df.loc[df[\"Date\"] == gf, \"is_holiday\"] = 1\n",
    "        year = year + 1\n",
    "    return df\n",
    "def create_test_frame(start_date, end_date, old_data, team):\n",
    "    rng = get_hoops_for_test_frame(start_date, end_date, team)\n",
    "    rng[\"Date\"] = pd.to_datetime(rng[\"Date\"])\n",
    "    rng[\"day_of_week\"] = rng[\"Date\"].dt.weekday\n",
    "    rng[\"month\"] = rng[\"Date\"].dt.month\n",
    "    rng[\"day_of_month\"] = rng[\"Date\"].dt.day\n",
    "    rng[\"year\"] = rng[\"Date\"].dt.year\n",
    "    us_holidays = holidays.US()\n",
    "    rng[\"is_holiday\"] = rng[\"Date\"].apply(lambda x: int(x in us_holidays))\n",
    "    rng[\"is_holiday\"] = rng[\"is_holiday\"].astype(int)\n",
    "    rng = set_easter_holiday(rng)\n",
    "    \n",
    "    rng['day_of_year'] = rng['Date'].map(lambda x: x.timetuple().tm_yday)\n",
    "    rng['year_sin'] = np.sin(2 * np.pi * rng['day_of_year'] / 365.25)\n",
    "    rng['year_cos'] = np.cos(2 * np.pi * rng['day_of_year'] / 365.25)\n",
    "    rng['rolling_avg_28d'] = old_data['rolling_avg_28d'].iloc[-28:].mean()\n",
    "    \n",
    "    rng[\"last_week\"] = rng.apply(lambda row: get_last_week(1, row[\"Date\"], old_data), axis=1)\n",
    "    rng[\"last_week_2\"] = rng.apply(lambda row: get_last_week(2, row[\"Date\"], old_data), axis=1)\n",
    "    rng[\"last_week_3\"] = rng.apply(lambda row: get_last_week(3, row[\"Date\"], old_data), axis=1)\n",
    "    \n",
    "    if \"last_year\" in old_data.columns:\n",
    "        rng[\"last_week_4\"] = rng.apply(lambda row: get_last_week(4, row[\"Date\"], old_data), axis=1)\n",
    "        rng[\"last_year\"] = rng.apply(lambda row: old_data[old_data.Date == row[\"Date\"] - timedelta(364)].Calls.item(), axis=1)\n",
    "        rng[\"last_year-1w\"] = rng.apply(lambda row: old_data[old_data.Date == row[\"Date\"] - timedelta(371)].Calls.item(), axis=1)\n",
    "        rng[\"last_year+1w\"] = rng.apply(lambda row: old_data[old_data.Date == row[\"Date\"] - timedelta(357)].Calls.item(), axis=1)\n",
    "        rng[\"current_avg\"] = (rng[\"last_week\"] + rng[\"last_week_2\"] + rng[\"last_week_3\"] + rng[\"last_week_4\"])/4\n",
    "        rng[\"historic_avg\"] = (rng[\"last_year\"] + rng[\"last_year-1w\"] + rng[\"last_year+1w\"])/3\n",
    "        rng[\"corr\"] = (rng[\"current_avg\"] - rng[\"historic_avg\"])/rng[\"historic_avg\"]\n",
    "        rng.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        rng = rng.fillna(0)\n",
    "        \n",
    "        train_3 = old_data[[\"last_year\", \"last_year-1w\", \"last_year+1w\"]]\n",
    "        test_3 = rng[[\"last_year\", \"last_year-1w\", \"last_year+1w\"]]\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(train_3)\n",
    "        test_pca = pca.transform(test_3)\n",
    "        \n",
    "        test_full = np.concatenate([test_pca, rng[[\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                                                    \"last_week\", \"last_week_2\", \"last_week_3\", \"last_week_4\", \n",
    "                                            \"corr\", \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]]], axis=1)\n",
    "        print(test_full) \n",
    "\n",
    "        return test_full, [\"PCA_0\", \"PCA_1\"] + [\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                                                \"last_week\", \"last_week_2\", \"last_week_3\", \"last_week_4\", \n",
    "                                                \"corr\", \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]\n",
    "    else:\n",
    "        test_full = rng[[\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                         \"last_week\", \"last_week_2\", \"last_week_3\", \"rolling_avg_7d\", \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]]\n",
    "        print(test_full) \n",
    "        return test_full, [\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                           \"last_week\", \"last_week_2\", \"last_week_3\", \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]\n",
    "    \n",
    "    \n",
    "def prep_for_ML(df):\n",
    "    train = df\n",
    "    train_3 = train[[\"last_year\", \"last_year-1w\", \"last_year+1w\"]]\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(train_3)\n",
    "    train_pca = pca.transform(train_3)\n",
    "    feature_cols = [\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                    \"last_week\", \"last_week_2\", \"last_week_3\", \"last_week_4\", \n",
    "                    \"corr\", \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]\n",
    "    train_full = np.concatenate([train_pca, train[feature_cols]], axis=1)\n",
    "    return train_full\n",
    "\n",
    "\n",
    "def pad_call_data(data):\n",
    "    start = data.Date.iloc[0]\n",
    "    end = data.Date.iloc[-1]\n",
    "    rng = pd.date_range(start=start, end=end).date\n",
    "    df = pd.DataFrame(rng, columns=['Date'])\n",
    "    df = pd.merge(df, data, how=\"outer\", on=\"Date\")\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "def run_query(sql):\n",
    "    reporting_client_db = get_db_Client_data_import()\n",
    "    cursor_insert = reporting_client_db.cursor()\n",
    "    cursor_insert.execute(sql)\n",
    "    reporting_client_db.commit()\n",
    "    cursor_insert.close()\n",
    "    \n",
    "def get_hoops_for_test_frame(start_date, end_date,team):\n",
    "    from datetime import date, timedelta, datetime\n",
    "    reporting_db = get_db_dictionary()\n",
    "\n",
    "    todays_date = datetime.today().date()\n",
    "    sql = \"\"\"Query to pull hoops\"\"\"\n",
    "    sql = sql % (team,start_date,end_date,team,start_date,end_date)\n",
    "    Hoops = pd.read_sql(sql, con=reporting_db)\n",
    "\n",
    "    rng = pd.DataFrame(Hoops, columns=['Date'])\n",
    "    return rng\n",
    "# # Retrieves the number of calls from the (depth) most recent week.\n",
    "# # For future dates, which haven't yet occurred, this function grabs call data from the most current \n",
    "# # weeks of data.\n",
    "def get_last_week(depth, day, old_data):\n",
    "    old_data['Date'] = pd.to_datetime(old_data['Date'])\n",
    "\n",
    "#     old_data['Date'] = old_data['Date'].dt.date\n",
    "    i = 0\n",
    "    while day > old_data.Date.iloc[-1]:\n",
    "        day = day - timedelta(7)\n",
    "    \n",
    "    d = day-timedelta( 7 * (depth-1) )\n",
    "    \n",
    "    return old_data[old_data.Date == d].Calls.item()\n",
    "\n",
    "def future_projection(start_date, end_date, client, team, model=None, percentage_increase=0.0):\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor(random_state=63)\n",
    "    \n",
    "    if isinstance(start_date, str):\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    elif isinstance(start_date, date):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"start_date must be a string or a date object\")\n",
    "    \n",
    "    df = get_call_data(client, team)\n",
    "    print(df)\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "    \n",
    "    q_low, q_high = df['Calls'].quantile([0.01, 0.99])\n",
    "    df['Calls'] = df['Calls'].clip(lower=q_low, upper=q_high)\n",
    "    \n",
    "    if \"last_year\" in df.columns and df.iloc[0].Date <= start_date - timedelta(371):\n",
    "        X = prep_for_ML(df)\n",
    "        y = df[\"Calls\"]\n",
    "        feature_cols = [\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                        \"last_week\", \"last_week_2\", \"last_week_3\", \"last_week_4\", \"corr\", \n",
    "                        \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]\n",
    "    else:\n",
    "        feature_cols = [\"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \n",
    "                        \"last_week\", \"last_week_2\", \"last_week_3\",\n",
    "                        \"year_sin\", \"year_cos\", \"rolling_avg_28d\"]\n",
    "        X = df[feature_cols]\n",
    "        y = df[\"Calls\"]\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50,75,100,125],\n",
    "        'max_depth': [3, 9, 12, 25],\n",
    "        'min_samples_split': [4, 8, 12],\n",
    "        'min_samples_leaf': [2, 6, 12]\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    mae_scores = cross_val_score(best_model, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "    rmse_scores = cross_val_score(best_model, X, y, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    avg_mae = -mae_scores.mean()\n",
    "    avg_rmse = -rmse_scores.mean()\n",
    "    print(f\"Cross-Validation MAE: {avg_mae:.2f}, RMSE: {avg_rmse:.2f}\")\n",
    "    \n",
    "    best_model.fit(X, y)\n",
    "    \n",
    "    test, test_feature_cols = create_test_frame(start_date, end_date, df, team)\n",
    "    rng = get_hoops_for_test_frame(start_date, end_date, team)\n",
    "    print(rng)\n",
    "\n",
    "    test_df = pd.DataFrame(test, columns=test_feature_cols)\n",
    "    test_df['Date'] = rng['Date'].values\n",
    "    pred = best_model.predict(test_df.drop(columns=['Date'], errors='ignore'))\n",
    "    print(pred)\n",
    "\n",
    "    # Apply percentage increase and clip negative values\n",
    "    adjusted_pred = pred * (1 + percentage_increase / 100)\n",
    "    adjusted_pred = np.maximum(adjusted_pred, 0)  # Ensure no negative predictions\n",
    "    \n",
    "    pred_df = pd.DataFrame(adjusted_pred, columns=[\"Predictions\"])\n",
    "    pred_df = pd.concat([pred_df.reset_index(drop=True), rng.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_yoy_call_volume_with_2025(preds, client_code, team=None, start_date=\"2025-01-01\", end_date=\"2025-12-31\"):\n",
    "    df_historical = get_call_data(client_code, team)\n",
    "#     df_historical = df_historical[df_historical['Date']<\"2025-02-01\"]\n",
    "    df_historical_max = df_historical['Date'].max() if not df_historical.empty else None\n",
    "    print(f\"Historical data: {len(df_historical)} rows, max date: {df_historical_max}\")\n",
    "    \n",
    "    if len(df_historical) == 0:\n",
    "        print(\"No historical data available to plot.\")\n",
    "        return\n",
    "    \n",
    "    df_pred = preds\n",
    "    print(f\"Predictions input: {len(df_pred)} rows\")\n",
    "    \n",
    "    if len(df_pred) > 0:\n",
    "        # Convert dates for consistency\n",
    "        df_historical['Date'] = pd.to_datetime(df_historical['Date']).dt.date\n",
    "        df_pred['Date'] = pd.to_datetime(df_pred['Date']).dt.date\n",
    "        \n",
    "        # Filter historical data to exclude dates in predictions\n",
    "        pred_dates = set(df_pred['Date'])\n",
    "        df_historical = df_historical[~df_historical['Date'].isin(pred_dates)]\n",
    "        print(f\"Filtered historical data: {len(df_historical)} rows after excluding prediction dates\")\n",
    "        \n",
    "        # Filter predictions to post-historical dates (optional, for safety)\n",
    "        df_historical_max = df_historical['Date'].max() if not df_historical.empty else None\n",
    "        if df_historical_max:\n",
    "            df_historical_max = pd.to_datetime(df_historical_max).date()\n",
    "            df_pred = df_pred[df_pred['Date'] > df_historical_max]\n",
    "        print(f\"Predictions after filter: {len(df_pred)} rows, min date: {df_pred['Date'].min()}\")\n",
    "        \n",
    "        df_pred = df_pred.rename(columns={'Predictions': 'Calls'})\n",
    "        df_combined = pd.concat([df_historical[['Date', 'Calls']], df_pred[['Date', 'Calls']]], ignore_index=True)\n",
    "    else:\n",
    "        print(\"No prediction data available. Plotting historical data only.\")\n",
    "        df_combined = df_historical[['Date', 'Calls']]\n",
    "    \n",
    "    print(f\"Combined data: {len(df_combined)} rows\")\n",
    "    df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
    "    df_combined['Year'] = df_combined['Date'].dt.year\n",
    "    df_combined['Month'] = df_combined['Date'].dt.month\n",
    "    print(f\"Years in combined data: {df_combined['Year'].unique()}\")\n",
    "    \n",
    "    monthly_data = df_combined.groupby(['Year', 'Month'])['Calls'].sum().reset_index()\n",
    "    pivot_data = monthly_data.pivot(index='Month', columns='Year', values='Calls').fillna(0)\n",
    "    print(f\"Pivot columns (years): {pivot_data.columns.tolist()}\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for year in pivot_data.columns:\n",
    "        plt.plot(pivot_data.index, pivot_data[year], marker='o', linestyle='-', label=str(year))\n",
    "        for month in pivot_data.index:\n",
    "            value = pivot_data.loc[month, year]\n",
    "            if value > 0:\n",
    "                plt.text(month, value, f'{int(value)}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.title(f'Year-over-Year Call Volume by Month (Client: {client_code}, Team: {team or \"All\"})')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Call Volume')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')  # Fixed typo 'Yeazr' to 'Year'\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = future_projection(ds, de, Clientcode, \"Technical Support\",percentage_increase=0)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = plot_yoy_call_volume_with_2025(preds,Clientcode, team=\"Technical Support\", start_date=ds, end_date=de)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952807f",
   "metadata": {},
   "source": [
    "### Avg Handle Time Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_data(client, team=None, skill=None):\n",
    "    avaya_db = get_db_avaya_stats()\n",
    "    \n",
    "    # Query for collecting a team's skill group call data\n",
    "    if  team:\n",
    "        sql = \"\"\"Query to pull Handle time for AHT predictions\"\"\"\n",
    "        sql = sql % (client, team)\n",
    "  \n",
    "    # Execute query, put results in dataframe\n",
    "    df = pd.read_sql(sql, con=avaya_db)\n",
    "    \n",
    "    df = df.sort_values(by='Date', ascending=True)\n",
    "    today_date = datetime.today().date()\n",
    "    df = df[df['Date'] != today_date]\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Turn the timedelta 'Time' column into integer representation of the time\n",
    "    df = time_to_int(df)\n",
    "    # This function makes a dataframe containing every time interval from 00:00 to 23:30 of every day, starting from the \n",
    "    # first day of collected data, to the last day of collected data. A complete time series with no gaps.\n",
    "    rng = get_date_range(df.Date.iloc[0], df.Date.iloc[-1])\n",
    "    # Merge the collected data, which contains gaps, with the gapless rng dataframe, to create one complete time series\n",
    "    df = pd.merge(rng, df, how=\"outer\", on=[\"Date\",\"Time\"])\n",
    "\n",
    "    # The places in which the collected data had gaps will leave a NaN after the merge, so we should fill these with\n",
    "    # a 0. \n",
    "    df = df.fillna(0)\n",
    "    df['Calls'] = df['Calls'].astype(int)\n",
    "\n",
    "    # This function adds all the columns used by the machine learning model\n",
    "    df = feature_engineering(df,team)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fff490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(first, last):\n",
    "    df = pd.DataFrame(columns=[\"Date\",\"Time\"])\n",
    "    rng = pd.date_range(start=first, end=last).date\n",
    "    rng = pd.DataFrame(rng, columns=['Date'])\n",
    "    for i in range(len(rng)):\n",
    "        d = rng.Date[i]\n",
    "        arr = []\n",
    "        for i in range(12):\n",
    "            arr.append(i*2*100)\n",
    "        temp = pd.DataFrame(arr, columns=[\"Time\"])\n",
    "        temp[\"Date\"] = d\n",
    "        df = pd.concat([df, temp])\n",
    "    df[\"Time\"]=df[\"Time\"].astype(int)\n",
    "#     print(df.dtypes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_AHT(df,team):\n",
    "    # Convert 'Date' column to datetime if not already in that format\n",
    "    Contractual_threshold = \"\"\"Select AHT_Contractual_threshold from reporting.client_report_conditions where clientCode = \"xyz\"  \"\"\"\n",
    "    Contractual_threshold = pd.read_sql(Contractual_threshold, con=avaya_db)\n",
    "    \n",
    "    Contractual_threshold = Contractual_threshold['AHT_Contractual_threshold'].loc[0]\n",
    "    Contractual_threshold\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Sort the DataFrame by 'Date'\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if team == \"Technical Support\":\n",
    "        # Identify rows where 'day_of_week' < 5 and 'Calls' is 0\n",
    "        weekday_calls = (df['day_of_week'] < 5)\n",
    "        df['Time'] = df['Time'].astype(int)\n",
    "\n",
    "        # Identify the time intervals from 6 pm to 8 pm with 'Calls' = 0\n",
    "        evening_calls_zero = (df['Time']== 2000) & (df['Calls'] == 0)\n",
    "\n",
    "        # Update identified intervals in the original DataFrame to 'Calls' = 1\n",
    "        df.loc[weekday_calls & evening_calls_zero, 'Calls'] = 1\n",
    "        df.loc[weekday_calls & evening_calls_zero, 'AvgHandleTime'] = Contractual_threshold\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['day_of_week'] < 5 and row['Calls'] == 0:\n",
    "            current_date = row['Date']\n",
    "            current_time = row['Time']\n",
    "\n",
    "            # Find the previous week's date for the same day (Wednesday in this case)\n",
    "            prev_week_same_day = current_date - pd.DateOffset(weeks=1)\n",
    "\n",
    "            # Filter data for the same day and time interval in the previous week\n",
    "            prev_week_data = df[\n",
    "                (df['Date'] == prev_week_same_day) &\n",
    "                (df['Time'] == current_time)\n",
    "            ]\n",
    "\n",
    "            # Check if there's data for the same time interval in the previous week\n",
    "            if not prev_week_data.empty:\n",
    "                # Replace missing 'Calls' values for the specific interval from the previous week's data\n",
    "                df.at[index, 'Calls'] = prev_week_data.iloc[0]['Calls']  # Using iloc[0] to get the first value if multiple found\n",
    "                df.at[index, 'AvgHandleTime'] = prev_week_data.iloc[0]['AvgHandleTime']  # Use the 'AHT' value from the previous week\n",
    "\n",
    "    df['Date'] = df['Date'].dt.date\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_engineering(df,team):\n",
    "\n",
    "    short = False\n",
    "    if df.iloc[0].Date > df.iloc[-1].Date - timedelta(371*2): short = True\n",
    "    # The day of the week for each row is important information\n",
    "    df[\"day_of_week\"] = df.apply(lambda row : row[\"Date\"].weekday(), axis=1)\n",
    "    df[\"AvgHandleTime\"].loc[df[\"day_of_week\"] > 4] = 0\n",
    "    \n",
    "    # Extract information about the date, because a date can not be fed to machine learning model. Only numbers.\n",
    "    df[\"month\"] = df.apply(lambda row: row[\"Date\"].month, axis = 1)\n",
    "    df[\"day_of_month\"] = df.apply(lambda row: row[\"Date\"].day, axis = 1)\n",
    "    df[\"year\"] = df.apply(lambda row: row[\"Date\"].year, axis = 1)\n",
    "    \n",
    "    #is date a US Holiday?\n",
    "    us_holidays = holidays.US()\n",
    "    df[\"is_holiday\"] = df.apply(lambda row : int(row[\"Date\"] in us_holidays), axis=1 )\n",
    "    df = set_easter_holiday(df)\n",
    "    df = fill_missing_AHT(df,team)\n",
    "\n",
    "    # call volume a week ago\n",
    "    df[\"last_week\"] = df[\"AvgHandleTime\"].shift(12*7)\n",
    "    # 2 weeks ago\n",
    "    df[\"last_week_2\"] = df[\"AvgHandleTime\"].shift(12*7*2)\n",
    "    # 3 weeks ago\n",
    "    df[\"last_week_3\"] = df[\"AvgHandleTime\"].shift(12*7*3)\n",
    "    \n",
    "    if not short:\n",
    "        # 4 weeks ago\n",
    "        df[\"last_week_4\"] = df[\"AvgHandleTime\"].shift(12*7*4)\n",
    "        # 5 weeks ago\n",
    "        df[\"last_week_5\"] = df[\"AvgHandleTime\"].shift(12*7*5)\n",
    "        # 6 weeks ago\n",
    "        df[\"last_week_6\"] = df[\"AvgHandleTime\"].shift(12*7*6)\n",
    "        # 7 weeks ago\n",
    "        df[\"last_week_7\"] = df[\"AvgHandleTime\"].shift(12*7*7)\n",
    "        # 8 weeks ago\n",
    "        df[\"last_week_8\"] = df[\"AvgHandleTime\"].shift(12*7*8)\n",
    "        # 9 weeks ago\n",
    "        df[\"last_week_9\"] = df[\"AvgHandleTime\"].shift(12*7*9)\n",
    "        # 364 days ago, because we want to land on the same day of the week, not same date\n",
    "        df[\"last_year\"] = df[\"AvgHandleTime\"].shift(12*364)\n",
    "        # same weekday from last year minus another week\n",
    "        df[\"last_year-1w\"] = df[\"AvgHandleTime\"].shift(12*371)\n",
    "        # same weekday from last year plus another week\n",
    "        df[\"last_year+1w\"] = df[\"AvgHandleTime\"].shift(12*357)\n",
    "    # Several rows of data will have null values for last_year and/or last_week columns so we drop them\n",
    "    df = df.dropna()\n",
    "#     df[\"AvgHandleTime\"].loc[df.Calls < 5] = 0\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_to_int(df):\n",
    "    df[\"Time\"] = df[\"Time\"].astype(str)\n",
    "    df[\"Time\"] = df.apply(lambda row : row[\"Time\"][-8:-3], axis=1)\n",
    "    df[\"Time\"] = df.apply(lambda row : int(row[\"Time\"].replace(\":\", \"\")), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_last_week(depth, day, time, old_data):\n",
    "    i = 0\n",
    "    while day > old_data.Date.iloc[-1]:\n",
    "        day = day - timedelta(7)\n",
    "    \n",
    "    d = day-timedelta( 7 * (depth-1) )\n",
    "    print (d,time)\n",
    "    print(old_data[(old_data.Date == d) & (old_data.Time == time)])\n",
    "    return old_data[(old_data.Date == d) & (old_data.Time == time)].AvgHandleTime.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03337ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hoops_for_test_frame(start_date, end_date,team):\n",
    "    from datetime import date, timedelta, datetime\n",
    "    import datetime as dt\n",
    "\n",
    "    reporting_db = get_db_dictionary()\n",
    "    \n",
    "    todays_date = datetime.today().date()\n",
    "    sql = \"\"\"Get HOOPS\"\"\"\n",
    "    sql = sql % (team,start_date,end_date,team,start_date,end_date)\n",
    "\n",
    "    Hoops = pd.read_sql(sql, con=reporting_db)\n",
    "    Hoops['Hoop_Start']= pd.to_datetime(Hoops['Hoop_Start'])\n",
    "    Hoops['Hoop_Start'] = pd.to_datetime(Hoops['Hoop_Start']).dt.time\n",
    "\n",
    "    Hoops['Hoop_End']= pd.to_datetime(Hoops['Hoop_End'])\n",
    "    Hoops['Hoop_End'] = pd.to_datetime(Hoops['Hoop_End']).dt.time\n",
    "\n",
    "    total_rows = Hoops.shape[0]\n",
    "\n",
    "    # total_rows = 15\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for x in range(total_rows):\n",
    "        PIT_Date = Hoops['PIT_Date'][x]\n",
    "        Hoop_Start = str(Hoops['Hoop_Start'][x])\n",
    "        Hoop_End = str(Hoops['Hoop_End'][x])\n",
    "\n",
    "        # Convert Hoop_Start and Hoop_End from strings to datetime objects\n",
    "        start = dt.datetime.strptime(Hoop_Start, '%H:%M:%S')\n",
    "        end = dt.datetime.strptime(Hoop_End, '%H:%M:%S')\n",
    "        delta = dt.timedelta(hours=2)\n",
    "\n",
    "        # Loop through the time range\n",
    "        t = start\n",
    "        while t < end:\n",
    "            Daterec = t.strftime('%H:%M:%S')\n",
    "            t += delta\n",
    "            # Append each row to the list as a dictionary\n",
    "            rows.append({'Date_PIT': PIT_Date, 'Timestamp': Daterec})\n",
    "\n",
    "    # Create the Final DataFrame from the list of dictionaries\n",
    "    Finaldataframe = pd.DataFrame(rows)\n",
    "\n",
    "    df1 = Finaldataframe\n",
    "    df1[\"Timestamp\"] = df1[\"Timestamp\"].astype(str)\n",
    "    df1[\"Timestamp\"] = df1.apply(lambda row : row[\"Timestamp\"][:-3], axis=1)\n",
    "    df1[\"Timestamp\"] = df1.apply(lambda row : row[\"Timestamp\"].replace(\":\", \"\"), axis=1)\n",
    "    df1[\"Timestamp\"] = df1[\"Timestamp\"].astype(int)\n",
    "    df1 = df1.rename(columns={'Timestamp': 'Time'})\n",
    "    df1 = df1.rename(columns={'Date_PIT': 'Date'})\n",
    "\n",
    "    return df1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c362fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_test_frame(start_date, end_date, old_data,team):\n",
    "    rng = get_hoops_for_test_frame(start_date, end_date,team)\n",
    "    \n",
    "    rng[\"day_of_week\"] = rng.apply(lambda row : row[\"Date\"].weekday(), axis=1)\n",
    "    rng[\"month\"] = rng.apply(lambda row: row[\"Date\"].month, axis = 1)\n",
    "    rng[\"day_of_month\"] = rng.apply(lambda row: row[\"Date\"].day, axis = 1)\n",
    "    rng[\"year\"] = rng.apply(lambda row: row[\"Date\"].year, axis = 1)\n",
    "    \n",
    "    us_holidays = holidays.US()\n",
    "    rng[\"is_holiday\"] = rng.apply(lambda row : row[\"Date\"] in us_holidays, axis=1 )\n",
    "    rng[\"is_holiday\"] = rng[\"is_holiday\"].astype(int)\n",
    "    rng = set_easter_holiday(rng)\n",
    "    \n",
    "    rng[\"last_week\"] = rng.apply(lambda row : get_last_week(1, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "    rng[\"last_week_2\"] = rng.apply(lambda row : get_last_week(2, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "    rng[\"last_week_3\"] = rng.apply(lambda row : get_last_week(3, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "\n",
    "    if \"last_year\" in old_data.columns:\n",
    "        rng[\"last_week_4\"] = rng.apply(lambda row : get_last_week(4, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_5\"] = rng.apply(lambda row : get_last_week(5, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_6\"] = rng.apply(lambda row : get_last_week(6, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_7\"] = rng.apply(lambda row : get_last_week(7, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_8\"] = rng.apply(lambda row : get_last_week(8, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        rng[\"last_week_9\"] = rng.apply(lambda row : get_last_week(9, row[\"Date\"], row[\"Time\"], old_data), axis=1)\n",
    "        \n",
    "        rng[\"last_year\"] = rng.apply(lambda row : old_data[(old_data.Date == row.Date - timedelta(364)) & (old_data.Time == row.Time)].AvgHandleTime.item(), axis=1)\n",
    "        rng[\"last_year-1w\"] = rng.apply(lambda row : old_data[(old_data.Date == row[\"Date\"] - timedelta(371)) & (old_data.Time == row.Time)].AvgHandleTime.item(), axis=1)\n",
    "        rng[\"last_year+1w\"] = rng.apply(lambda row : old_data[(old_data.Date == row[\"Date\"] - timedelta(357)) & (old_data.Time == row.Time)].AvgHandleTime.item(), axis=1)\n",
    "        \n",
    "        rng = rng[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \"last_week_2\", \"last_week_3\", \"last_week_4\", \"last_week_5\", \"last_week_6\", \"last_week_7\", \"last_week_8\", \"last_week_9\", \"last_year\", \"last_year-1w\", \"last_year+1w\"]]\n",
    "    else:\n",
    "        rng = rng[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \"last_week_2\", \"last_week_3\"]]\n",
    "    return rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def future_projection(start_date, end_date, client, team, model):\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    elif isinstance(start_date, date):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"start_date must be a string or a date object\")\n",
    "        \n",
    "    df = get_call_data(client, team)\n",
    "    if len(df) == 0: \n",
    "        return []\n",
    "    if df.iloc[-1].Date < date.today() - timedelta(21):\n",
    "        return []\n",
    "    print(\"\\tFitting model with historic data...\")\n",
    "  \n",
    "    if \"last_year\" in df.columns and df.iloc[0].Date <= start_date - timedelta(371):\n",
    "       \n",
    "        model.fit(df[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \n",
    "                         \"last_week_2\", \"last_week_3\", \"last_week_4\", \"last_week_5\", \"last_week_6\", \n",
    "                         \"last_week_7\", \"last_week_8\", \"last_week_9\", \"last_year\", \"last_year-1w\", \n",
    "                         \"last_year+1w\"]], df[\"AvgHandleTime\"])\n",
    "\n",
    "    else:\n",
    "     \n",
    "        model.fit(df[[\"Time\", \"day_of_week\", \"month\", \"day_of_month\", \"year\", \"is_holiday\", \"last_week\", \n",
    "                         \"last_week_2\", \"last_week_3\"]], df[\"AvgHandleTime\"])\n",
    "\n",
    "    test = create_test_frame(start_date, end_date, df,team)\n",
    "    \n",
    "    print(\"\\tGenerating predictions...\")\n",
    "    pred = model.predict(test)\n",
    "    pred = pd.DataFrame(pred, columns=[\"Predictions\"])\n",
    "    \n",
    "    rng = get_hoops_for_test_frame(start_date, end_date,team)\n",
    "    rng[\"day_of_week\"] = rng.apply(lambda row : row.Date.weekday(), axis=1)\n",
    "    pred = pd.concat([pred.reset_index(drop=True), rng.reset_index(drop=True)], axis=1)\n",
    "    pred[\"Predictions\"].loc[pred[\"day_of_week\"] > 4] = 0\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c066a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Update statement exists to set the AHT column of an existing row in the DB to the projected value for that row \n",
    "update_sql = \"\"\"UPDATE query\"\"\"\n",
    "\n",
    "# For each team in the teams array, get the prediction dataframe, and perform the update, and insert queries\n",
    "for team in teams:\n",
    "    print(\"Generating AHT Predictions for\",team, \"...\")\n",
    "    # Get the prediction dataframe for team, then execute update_sql for team\n",
    "    pred = future_projection(ds, de, client, team, model)\n",
    "    if len(pred) == 0:\n",
    "        print(\"\\tDue to lack of data, \" + team + \" is being skipped\")\n",
    "        continue\n",
    "    print(\"\\tInserting predictions into the database\")\n",
    "    \n",
    "    for index, row in pred.iterrows():\n",
    "        if pred.iloc[index, 0] == 0:\n",
    "            continue\n",
    "        sql = update_sql\n",
    "        sql = sql % (pred.iloc[index, 0],client, team,VersionType, str(pred.iloc[index, 1]), pred.iloc[index, 2] * 100, pred.iloc[index, 2]*100 + 13000)\n",
    "        run_query(sql)\n",
    "    \n",
    "    print(\"\\tFinished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed27aa7",
   "metadata": {},
   "source": [
    "#### Erlang Script to Use Call arrival Pattern, Call Volume Projections, Avg Handle time Projections to Find FTE Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7223558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_int(df):\n",
    "    df[\"Time\"] = df[\"Time\"].astype(str)\n",
    "    df[\"Time\"] = df.apply(lambda row : row[\"Time\"][-8:-3], axis=1)\n",
    "    df[\"Time\"] = df.apply(lambda row : int(row[\"Time\"].replace(\":\", \"\")), axis=1)\n",
    "    return df\n",
    "\n",
    "def run_query(sql):\n",
    "    reporting_client_db = get_db_Client_data_import()\n",
    "    cursor_insert = reporting_client_db.cursor()\n",
    "    cursor_insert.execute(sql)\n",
    "    reporting_client_db.commit()\n",
    "    cursor_insert.close()\n",
    "    \n",
    "# Inserts the given fte into the database. Sets the team column to PARAM(team)\n",
    "def insert(client,fte,team):\n",
    "    insert_sql = \"\"\"INSERT INTO a.b(Pkey,Team,ContactType,Version_date,VersionType,Cap_Date,Cap_Time,input_Calls,clientCode,input_asa_seconds,input_aht_seconds,input_shrinkage_pct,input_min_service_level,input_max_occupancy,positions_fte_raw,positions_fte_shrink,serv_level_projection,occupancy_projection,waiting_probability,Smoothed_FTE1)\n",
    "    VALUES \"\"\"\n",
    "    D = \"\"\"ON DUPLICATE KEY UPDATE Version_date = Values(Version_date),input_aht_seconds=Values(input_aht_seconds),positions_fte_raw=Values(positions_fte_raw),\\n\n",
    "        input_Calls= Values(input_Calls),positions_fte_shrink=Values(positions_fte_shrink) ,serv_level_projection=Values(serv_level_projection),\\n\n",
    "        occupancy_projection=Values(occupancy_projection),waiting_probability=Values(waiting_probability),Smoothed_FTE1=Values(Smoothed_FTE1)\"\"\"\n",
    "#     team=\"Combined\"\n",
    "    i = 0\n",
    "    sql = insert_sql\n",
    "    print(sql)\n",
    "    version_date = str(datetime.now())\n",
    "    \n",
    "    for i, r in fte.iterrows():\n",
    "        \n",
    "        pkey = str(client)+ \"-\" + str(team)+ \"-\" + str(r.Date)+ \"-\" + str(r.Time*100) +\"-\"+ str(VersionType)\n",
    "        print(pkey)\n",
    "        s = \"('%s','%s', 'phone', '%s','%s', '%s', %s, %s, '%s', %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s),\\n\"\n",
    "        s = s % (pkey,team, version_date,VersionType, str(r.Date), r.Time*100, r.Calls,client, 30, r.aht, 0.35, 0.8, 0.85, r.raw_positions, r.positions, r.service_level, r.occupancy, r.waiting_prob, r.smooth)\n",
    "        sql += s\n",
    "        i += 1\n",
    "        if i == 500:\n",
    "            sql = sql[:-2]\n",
    "            sql += \"\\n\" + D\n",
    "            run_query(sql)\n",
    "            i = 0\n",
    "            sql = insert_sql\n",
    "    \n",
    "    # Unless the length of the dataframe is a multiple of 500, then there will be rows of the dataframe that\n",
    "    # have not been inserted into the DB. Here we insert the remaining rows, if they exist.\n",
    "    if sql != insert_sql:\n",
    "        sql = sql[:-2]\n",
    "        sql += \"\\n\" + D\n",
    "        run_query(sql)\n",
    "        \n",
    "        \n",
    "def roll(fte):\n",
    "    # Sort the dataframe by Date and Time\n",
    "    fte_sorted = fte.sort_values(by=['Date', 'Time']).reset_index(drop=True)\n",
    "    \n",
    "    # Initialize an empty list to store smoothed values\n",
    "    smooth_values = []\n",
    "    \n",
    "    # Loop over each unique day in the Date column\n",
    "    for d in fte_sorted['Date'].unique():\n",
    "        day_fte = fte_sorted[fte_sorted['Date'] == d]\n",
    "        \n",
    "        # Perform rolling mean on the positions column\n",
    "        smooth_day = day_fte['positions'].rolling(4, center=True, min_periods=1).mean().astype(int)\n",
    "        \n",
    "        # Append smoothed values to the list\n",
    "        smooth_values.extend(smooth_day.tolist())\n",
    "    \n",
    "    # Add the smoothed values to a new column in the original dataframe\n",
    "    fte_sorted['smooth'] = smooth_values\n",
    "    \n",
    "    return fte_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcefe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if client_projection_calls == 'Y' :\n",
    "Client_CVP = \"\"\"\n",
    "    SELECT Projection_Date AS Date, Calls\n",
    "    FROM w.f\n",
    "    WHERE Type = 'P' AND Client = '%s' AND Team ='%s'\n",
    "      AND Version_Date = (\n",
    "          SELECT MAX(Version_Date) \n",
    "          FROM w.f\n",
    "          WHERE Type = 'P' AND Client = '%s' AND Team = '%s'\n",
    "      )\n",
    "      AND Projection_Date >= '%s' AND Projection_Date <=' %s'\n",
    "      AND Pkey != ''\n",
    "\"\"\"\n",
    "# else:\n",
    "    \n",
    "\n",
    "sql_cvp = \"\"\"\n",
    "SELECT Projection_Date AS Date, Calls, callsByClient\n",
    "FROM w.CVP\n",
    "WHERE Version_Date = (\n",
    "    SELECT MAX(Version_Date)\n",
    "    FROM w.CVP\n",
    "    WHERE Client = '%s' AND Team = '%s' AND VersionType = '%s' AND Projection_Date >= '%s' AND Projection_Date <= '%s' AND Pkey != ''\n",
    ") AND Client = '%s' AND Team = '%s' AND VersionType = '%s' AND Projection_Date >= '%s' AND Projection_Date <= '%s' AND Pkey != ''\n",
    "\"\"\"\n",
    "\n",
    "# This query selects CAP data for a team, for all dates between ds and de, inclusive.\n",
    "sql_cap = \"\"\"\n",
    "SELECT Cap_Date AS Date, Cap_Time AS Time, CAP_Percent1 AS Percent, AvgHandleTime\n",
    "FROM workforce.CAP\n",
    "WHERE Version_timestmp = (\n",
    "    SELECT MAX(Version_timestmp)\n",
    "    FROM workforce.CAP\n",
    "    WHERE clientCode = '%s' AND Team = '%s' AND VersionType = '%s' AND Cap_Date >= '%s' AND Cap_Date <= '%s' AND Pkey != ''\n",
    ") AND clientCode = '%s' AND Team = '%s' AND VersionType = '%s' AND Cap_Date >= '%s' AND Cap_Date <= '%s' AND Pkey != ''\n",
    "\"\"\"\n",
    "\n",
    "if client == \"xyz\":\n",
    "    teams = ['a' , 'b']\n",
    "    \n",
    "reporting_db = get_db_dictionary()\n",
    "Client_condition = \"\"\"Select clientCode,ASA_Contractual_threshold from a.client_report_conditions\n",
    "                        where clientCode = '%s'\"\"\"\n",
    "Client_condition = Client_condition % (client)\n",
    "Client_condition = pd.read_sql(Client_condition,con= reporting_db)\n",
    "\n",
    "ASA_value = Client_condition['ASA_Contractual_threshold'].iloc[0]\n",
    "ASA_value = ASA_value / 60\n",
    "ASA_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VersionType = 'P'\n",
    "reporting_client_db = get_db_Client_data_import()\n",
    "\n",
    "# This will contain non-ASA FTE projections. It will be used to create a combined FTE projection for SOP, SQW, and Operator\n",
    "for team in teams:\n",
    "    print(team)\n",
    "    \n",
    "    # Initialize a list to accumulate rows for the fte projections\n",
    "    fte_data = []\n",
    "    \n",
    "    # Fill in the SQL query with specific values\n",
    "    sql = sql_cvp % (client, team, VersionType, str(ds), str(de), client, team, VersionType, str(ds), str(de))\n",
    "    cvp = pd.read_sql(sql, con=reporting_client_db)\n",
    "    \n",
    "    Client_CVP_PD = Client_CVP % (client, team, client,team,str(ds), str(de))\n",
    "    Client_CVP_PD = pd.read_sql(Client_CVP_PD, con=reporting_client_db)\n",
    "    print(\"Client provided calls:\" )\n",
    "    print(Client_CVP_PD)\n",
    "    \n",
    "    # Create a mapping Series from df2\n",
    "    calls_map = Client_CVP_PD.set_index('Date')['Calls']\n",
    "\n",
    "    # Update 'Calls' in df1 using map from the 'Date' column\n",
    "    cvp['Calls'] = cvp['Date'].map(calls_map).fillna(cvp['Calls'])\n",
    "    print(cvp)\n",
    "    \n",
    "    # Fill in the SQL query for cap\n",
    "    sql = sql_cap % (client, team, VersionType, str(ds), str(de), client, team, VersionType, str(ds), str(de))\n",
    "    cap = pd.read_sql(sql, con=reporting_client_db)\n",
    "    \n",
    "    # Iterate over cap dataframe\n",
    "    for i, row in cap.iterrows():\n",
    "        print(f\"Processing row {i} for team {team}\")\n",
    "        \n",
    "        if row.AvgHandleTime == 0:\n",
    "            continue  # Skip intervals with AvgHandleTime of 0\n",
    "        \n",
    "        # Calculate target transactions\n",
    "        target = row.Percent * cvp[cvp.Date == row.Date].Calls.item()\n",
    "        print(f\"Target transactions: {target}\")\n",
    "        \n",
    "        # Create ErlangC object\n",
    "        erlang = ErlangC(transactions=target, asa=ASA_value, aht=row.AvgHandleTime / 60, interval=30, shrinkage=0.35)\n",
    "        req = erlang.required_positions(service_level=0.8, max_occupancy=0.85)\n",
    "        \n",
    "        # Adjust shrinkage if occupancy is too low\n",
    "        if req['occupancy'] < 0.6:\n",
    "            erlang = ErlangC(transactions=target, asa=ASA_value, aht=row.AvgHandleTime / 60, interval=30, shrinkage=0.25)\n",
    "            req = erlang.required_positions(service_level=0.8, max_occupancy=0.85)\n",
    "        \n",
    "        # Create a new row of data\n",
    "        r = {\n",
    "            'Date': row.Date,\n",
    "            'Time': row.Time,\n",
    "            'positions': req['positions'],\n",
    "            'raw_positions': req['raw_positions'],\n",
    "            'Calls': target,\n",
    "            'aht': row.AvgHandleTime,\n",
    "            'service_level': req['service_level'],\n",
    "            'occupancy': req['occupancy'],\n",
    "            'waiting_prob': req['waiting_probability']\n",
    "        }\n",
    "        \n",
    "        fte_data.append(r)\n",
    "    \n",
    "    # Convert the list of rows to a DataFrame\n",
    "    fte = pd.DataFrame(fte_data)\n",
    "    \n",
    "    # Apply time_to_int function\n",
    "    fte = time_to_int(fte)\n",
    "    \n",
    "    # Perform rolling mean smoothing\n",
    "    fte = roll(fte)\n",
    "    \n",
    "    # Insert the fte data into the database\n",
    "    print(f\"Final FTE data for {team}:\")\n",
    "    print(fte)\n",
    "    \n",
    "    insert(client, fte, team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62324090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitations\n",
    "# Projections are most accurate for up to 9 months into the future.\n",
    "# Requires sufficient historical data (at least 371 days for full feature set including last-year metrics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9a756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
